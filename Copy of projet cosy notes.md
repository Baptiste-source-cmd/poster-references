# **ETAT DE L’ART**

1. # **Introduction**

L'аnalyse de la cоmpleхité linguistique sur le trаitement autоmаtique dеs langues еt la cоmpréhensiоn du langаge, jоue un rôle clé dans les reсherсhеs cоntеmpоrainеs  Dаns le cadrе du prоjеt CoSy, nоtre attentiоn se cоncentre plus partiсulièrement sur la cоmpleхité syntaхique, qui englоbе les aspects structurels rеndant une phrаse plus оu mоins fаcile à prоduire оu à cоmprendrе. Pоur miеuх соnteхtualiser nоtre étudе, il est essеntiel d'eхaminer les diverses pеrspеctives sur la cоmpleхité telles que la syntaхe, la morphоlоgie, le leхique, la sémantique et la psyсhоlinguistiquе, ainsi quе les оutils de traitement autоmatiquе qui permettеnt d'évaluer сеs dimensiоns.

Cet état de l’art оffre ainsi un аperçu dеs recherсhes mаjеurеs cоncеrnant la соmpleхité syntaхiquе et ses méthоdes dе mеsure, lеs faсteurs psychоlinguistiques en liеn avеc lе cоût de traitement, lеs appоrts du traitement аutоmаtique dеs lаngues, nоtamment en сe qui cоncerne lа perpleхité et l'аnnоtаtiоn UD, et enfin les pоssibles relatiоns entre cоmpleхité, lisibilité et traitement du langage.

2. # **Définition de la complexité linguistique** 

   1. ## Cadres conceptuels généraux 

La complexité est un phénomène linguistique très particulier, et qui n’est pas exactement la complexité au sens grammatical. En effet, elle désigne la difficulté de compréhension associée aux structures et aux unités d’une langue. Elle est généralement considérée comme relative, plutôt que absolue.

 Pendant très longtemps, il était pensé que la complexité était quelque chose de plus ou moins égal dans les langues, Hockett (1958) appelle ça l’hypothèse « d’équi-complexité » qui est maintenant plus que remise en question, voire complètement falsifiée (Bentz et al., 2023). Tandis que Dahl (2004) penche plutôt vers l’acception que la complexité est liée à la longueur et la description nécessaire pour décrire un système linguistique, c’est le concept à partir duquel se forme l’idée qu’un élément linguistique se construit à partir de blocs concrets à plusieurs échelles dans une analyse micro-contextuelle, cela peut être des morphèmes, des mots, des syntagmes, des phrases, et tous ces éléments sont sujets à la complexité, en fonction de la manière dont ils s’assemblent (Romero-Barranco, 2020). 

2. ## Perspectives cognitives sur la complexité 

On retrouve donc plusieurs notions clés dans le concept de complexité, la première qui est évidente, c’est le coût cognitif de l’encodage du système linguistique et de ses structures, et son acquisition en tant qu’apprenant (Burov, 2022). Ce que ça suggère, c’est qu’en effet, pour un apprenant, plus une structure lui semble accessible, plus il y a de chances qu’il les apprenne rapidement, et durablement. Tandis qu’à l’inverse, plus le système est jugé compliqué cognitivement, plus il sera difficile de le comprendre. Non pas par difficulté intrinsèque, mais par un coût cognitif trop élevé, le système linguistique étant trop éloigné de ce que l’individu connaît. La réelle difficulté de la complexité est que finalement, seuls l’aspect morpho-syntaxique et lexicographique ne suffisent pas à bien la définir, elle peut aussi prendre différents angles, comme la complexité phonologique, un système comme le japonais qui est « syllabique » (plus ou moins) ne peut pas être comparé à un système comme le français. La complexité peut aussi être l’appropriation des sons, comme les clics, ou encore la prosodie. Notre intérêt ici porte donc essentiellement sur la syntaxe, mais il est à prendre en compte que la complexité linguistique est extrêmement riche et variée (Burov, 2022). On remarque néanmoins qu’il est plus fréquent de trouver des études sur ces aspects bien définis, que sur des représentations plus abstraites de la linguistique, telles que la pragmatique ou la phonologie, suggérant donc que si la complexité peut être employée dans de nombreux paradigmes linguistiques, il s’avère néanmoins qu’elle s’appuie essentiellement sur la morpho-syntaxe et l’analyse lexicale, pour des raisons d’abord historiques, mais également de simplicité. 

3. # **Modélisation de la complexité syntaxique**

   1. ## Mesures et limites des indices syntaxique    

L'intérêt de la syntaxe réside dans sa capacité à fournir des analyses facilement quantifiables. Parmi les critères syntaxiques pertinents (Lu, 2010), on distingue plusieurs types de mesures : 

* **Type 1 :** Longueur des unités de production avec des indicateurs comme  la longueur moyenne des propositions (MLC), des phrases (MLS) et des T-unit (MLT).  
* **Type 2 :** Complexité phrastique mesurée par le rapport entre le nombre de clauses et le nombre de phrases (C/S)  
* **Type 3 :** Subordination, évaluée par le rapport entre les T-units et le nombre de clauses (C/T), le nombre de T-units complexes et de T-units (CT/T), et le nombre de clauses dépendantes par rapport aux clauses (DC/C) ou aux T-units (DC/T).  
*  **Type 4 :** Coordination, mesurée avec les ratios comme le nombre de groupes coordonnés par clause (CP/C) ou par T-units (CP/T), et le rapport entre le nombre de T-units et de phrases (T/S).  
* **Type 5 :** Structures particulières, comprenant le nombre de groupes nominaux complexes par clause (CN/C) ou par T-units (CN/T), et le nombre de groupes verbaux par T-units (VP/T). 

* ## *Types issus du tableau n°1 de Lu, 2010*

Bien que cette liste (inspirée du tableau de Lu, 2010\) soit déjà assez complète,  elle n’est cependant pas exhaustive, elle ne couvre pas tous les aspects possibles. Elle ne prend pas en considération des critères tels que la rareté du lexique, les dépendances, le nombre de règles grammaticales, la répétabilité et la richesse morphémique. De plus, elle ne tient pas compte des éléments sémantico-pragmatiques qui sont très difficiles à quantifier et mesurer. Malgré ces limitations, elle offre un bon modèle de mesures pertinentes à observer. 

2. ## Application de la complexité  

Concernant les applications de la complexité dans le traitement automatique des langues, les enjeux bien que similaires, sont quelque peu différents, pour des raisons d’applicabilité, il y a de nombreuses facettes de la complexité qui ne sont pas abordables, notamment l’ambiguïté au sein des relatives (Hudelot, 1980\) montre que ce n’est probablement pas possible pour une machine de traiter correctement ce type de données pour les préparer de manière efficiente pour les apprenants, c’est un traitement que l’on fait très bien dans sa langue maternelle, en raison du grand nombre d’inférences langagières dont on dispose, mais ça n’est pas vraiment applicable, ni à des apprenants, ni à une machine. Néanmoins, on remarque qu’il y a des travaux spécifiquement sur l’ajustement de la complexité pour les apprenants, en effet, un domaine majeur de la complexité, notamment en TAL, est la didactique et l’adaptation de contenu numérique à des apprenants, notamment proposer des textes adaptés à l’âge ou au niveau d’éducation des lecteurs, c’est notamment l’enjeu autour de l’outil ALSI, dont nous reparlerons plus tard, de pouvoir « créer du contenu didactique ayant un niveau linguistique approprié, ou qui favorise l’apprentissage de certains objets de savoir en français » (Loignon, 2021). Il revient donc sur l’importance de la lisibilité pour la classification textuelle, mais souligne également que la mesure de complexité est essentielle à tout type d’apprentissage langagier, il postule que plus les outils seront efficaces pour l’analyse de la complexité, plus les outils d’apprentissage seront potentiellement ludiques et efficaces. La notion de complexité est donc aujourd’hui essentielle en linguistique, que cela soit pour les humanités numériques, ou la didactique, il est primordial aujourd’hui d’avoir des outils pour analyser la complexité linguistique, que cela soit pour les LLM, les supports automatisés, les différents assistants en ligne, les exerciseurs “automatisés” etc. La barrière de la complexité sémantique persiste encore, et il y a toujours peu d’outils performants pour la travailler, et il est observé que même les architectures les plus pointues ne savent pas du tout comment gérer une phrase complexe dès lors qu’il y a une ambiguïté qui n’est pas résoluble avec peu de descripteurs.(Antoine et al., 2024\) 

3. ## les modèles théoriques de la structure syntaxiques

Cela fait des années que les linguistes s'interrogent sur la difficulté de traitement des phrases : pourquoi certaines phrases, bien que correctes grammaticalement, posent-elles plus de problèmes que d'autres ? En effet, la complexité ne se mesure pas spécifiquement à la quantité de mots, mais à leur agencement dans une phrase ou une T-unit. La complexité syntaxique devient alors une notion essentielle pour analyser la compréhension, la production des phrases et la manière de les manipuler. Les travaux de Miller(1956) montrent que la mémoire de travail humaine ne peut gérer qu’un nombre limité d'éléments à la fois. Yngve(1960) applique cette limite au langage, il émet l’hypothèse qu'une phrase devient difficile lorsqu’elle requiert une rétention d’information trop importante simultanément. C’est le cas notamment dans les phrases très enchâssées. Par exemple, dans le cas d’une phrase simple: “ Le chat mange la souris” Phrase enchâssée : “ le chat que le chien poursuit mange la souris “ Phrases très enchâssées :” le chat que le chien que le fermier nourrit poursuit la souris. Plus la phrase s'enfonce dans des structures imbriquées. plus elle sollicite la mémoire. Chomsky et Miller distinguent entre la compétence(connaissance de la langue) et la performance(utilisation réelle de cette langue). Certaines phrases sont grammaticalement correctes, mais difficiles à traiter en pratique, car elles dépassent les limites de la performance, c’est-à-dire les limites de la mémoire, de la capacité des locuteurs à traiter les phrases trop longues ou trop complexes. 

1. ### L’analyse par constituance

Chomsky propose une analyse en constituants qui suppose que les phrases sont organisées en groupe(syntagmes) qui s'emboîtent. Ce modèle repose sur des règles de réécriture et génère des structures arborescentes. En revanche, certains phénomènes montrent les limites de ce système. Premièrement, la coordination, dans une séquence coordonnée telle que “cinéma et théâtre”, les deux éléments partagent le même statut et forment une unité comme une méthode classique de hiérarchisation ne rend vraiment compte, ce qui brouille l'idée d'un groupe dominé par une tête unique. De la même manière, les transformations comme le passage de l’actif au passif révèlent que les constituants changent le rôle grammatical sans que leur forme interne ne soit modifiée, ce qu’un simple découpage en groupe ne peut pas expliquer. Ces phénomènes montrent que la syntaxe ne peut pas être réduite à un assemblage statique de groupe et qu’il faut intégrer des relations fonctionnelles et des opérations plus profondes pour rendre compte du fonctionnement réel des phrases. Pour résoudre ces limites, Chomsky(1963) distingue: une structure profonde et régulière une structure de surface qui est obtenue après transformations une phrase peut sembler complexe en surface tout en gardant une structure profonde et simple, ce qui influence la facilité de traitement. 

4. ## les relations de dépendances et le flux des dépendances

Au lieu de représenter des groupes, les modèles des dépendances décrivent les liens entre les mots(un gouverneur → un dépendant). Cette approche est plus adaptée pour mesurer la complexité cognitive. Kahane (2001) propose le flux de dépendances qui est l’ensemble des liens syntaxiques qu’un lecteur doit garder actifs en mémoire à un point de la phrase. ce flux se mesure mot par mot, d’une côté par rapport à la taille du flux(le nombre total de dépendances actives) et de l’autre côté par rapport au poids du flux(le nombre de dépendances indépendantes entre elles, qui représente un mesure plus fidèle du coût cognitif) Certains configurations augmentent la complexité Enchâssements: apparaît lorsqu’une structure est insérée à l'intérieur d’une autre, comme une proposition relative ou une subordonnée. Plus il y a de niveaux emboîtés, plus la phrase devient difficile à analyser, car chaque dépendance doit être gardée en mémoire jusqu'à sa fermeture dépendances disjointes: il s’agit de relations syntaxiques entre deux éléments éloignés dans la phrase. Comme les mots liese ne sont pas voisins, le lecteur doit maintenir la connexion mentalement, ce qui accroît la complexité du traitement des croisements: les croisement surviennent quand deux paires de mots en relation s’entrecroisent dans la structure linéaire de la phrase. ce type d’organisation n’est pas aligné avec l’ordre en arbre habituel et impose un traitement plus lourd, car les dépendances ne peuvent plus être représentés de manière simple et hiérarchique et d’autres la réduisent comme les bouquets (plusieurs dépendants du même mot → information factorisée) selon Gibson(1990), il distingue deux coûts principaux coût de stockage: c’est ce que l’on doit garder en mémoire en attendant la suite des phrases. Les enchâssement augmentent ce coût coût d'intégration: c’est l’effort pour relier deux mots dépendants éloignés dans la phrase; plus la phrase est grande, plus l'intégration est coûteuse. Cette idée rejoint les travaux de Liu(2008) sur la longueur des dépendances.

4. # **la complexité selon les langues**  

Les langues ne structurent pas les phrases de la même manière. Dans les langues à verbe finale comme le japonais, les dépendances se maintiennent à la fin. d'autre part , dans les langues comme le français, les dépendances sont plus répartis. Back et al montrent que les dépendances croisées sont plus faciles et les dépendances enchâssées sont plus difficiles. ce contraste semble universel. La langue parlée présente des phrases plus courtes, avec moins d'enchâssement, et avec des hésitations, répétitions et amorces. Ces disfluences modifient localement la structure mais n'augmentent pas vraiment la complexité cognitive , car elles sont redondantes. cela explique pourquoi les structures profondes du français parlé sont généralement plus simples que celles de l'écrit les études menées sur les treebanks Universal dependencies montrent que: le poids de flux dépasse rarement 3 même dans les cas extrêmes, il ne dépasse presque jamais 6 ce phénomène apparaît dans toutes les langues étudiées Ces résultats confirment l'existence de contraintes universelles liées à la mémoire humaine. Il convient alors de se demander quels facteurs rendent une phrase complexe. Les recherches de Kahane et al. convergent notamment vers une liste plus ou moins stable de facteurs; nombre d'enchâssement longueur des dépendances croisements poid du flux ( relations indépendantes a gerer simultanement) complexité lexicale 

5. # **Analyse computationnelle de la complexité**

L'analyse de la complexité lexicale peut alors s'effectuer au travers de deux perspectives: psycholinguistique/pédagogique (décision lexicale, catégorisation sémantique) et traitement automatique du langage (simplification automatique des textes). L'idée principale est qu'un mot peut être perçu comme plus ou moins complexe en fonction de critères cognitifs, linguistiques et statistiques. Des éléments tels que la fréquence, la familiarité, l'âge d'acquisition, la longueur, la morphologie, la phonologie et la transparence entre graphèmes et phonèmes jouent un rôle essentiel dans l'évaluation de la difficulté des mots pour divers publics. Pour le projet CoSy, ces critères lexicaux ont une importance particulière pour saisir comment la complexité lexical interagit avec la complexité syntaxique, surtout en ce qui concerne l'annotation et le traitement automatique. 

1. ## Annotation syntaxiques et modèle UD  

L'annotation Universal Dependencies (UD) établit un cadre normalisé pour l'annotation de la morphosyntaxe des langues, mettant l'accent sur les mots et leurs relations grammaticales fondamentales telles que les sujets, les objets, les modificateurs et les compléments, tout en intégrant des informations morphologiques ainsi que des catégories grammaticales (Marie-Catherine de Marneffe et al., 2021). Ce qui distingue UD, c'est son approche interlinguistique, avec plus de 180 treebanks qui couvrent 104 langues, facilitant ainsi des comparaisons fiables et reproductibles entre différentes langues. Cette standardisation est bénéfique pour le parsing automatique, mais également pour l'analyse linguistique qualitative et comparative. Le cadre UD permet d'annoter les dépendances syntaxiques à divers niveaux, allant des dépendances de surface aux dépendances profondes, ce qui reflète plus fidèlement la structure réelle et le sens des phrases. Pour notre projet, UD constitue un outil indispensable pour évaluer la complexité syntaxique des phrases au sein d'un corpus ; garantir la cohérence et la reproductibilité des annotations ; préparer des données exploitables par des logiciels d'analyse automatique comme SpaCy ou Stanza. En somme, UD combine standardisation, robustesse et accessibilité, devenant ainsi une référence essentielle pour tout projet axé sur l'analyse syntaxique de corpus. 

2. ## Outils algorithmiques et perplexité 

En traitement automatique du langage, la perplexité est une mesure qui permet d'évaluer à quel point un modèle de langage trouve une phrase prévisible ou surprenante. Même si ce n’est pas un indicateur directement lié à la complexité syntaxique au sens linguistique (comme le nombre de subordonnées ou la profondeur de l’arbre syntaxique), elle peut néanmoins donner des indices intéressants. Par exemple, une phrase très simple comme « Je vais à l’école » est généralement bien prédite par un modèle, ce qui se traduit par une perplexité basse. À l’inverse, une phrase plus longue et structurée, du type « La proposition que l’on avait pourtant envisagée avant que la discussion ne prenne un tout autre tournant n’a finalement pas été retenue », crée plus d’incertitude pour le modèle. La présence d’une relative enchâssée, d’un changement de thème en cours de phrase, et d’un vocabulaire plus rare augmente la perplexité. Dans ce sens, la perplexité peut refléter, de manière indirecte, certaines formes de complexité syntaxique, notamment lorsque les constructions deviennent moins fréquentes ou plus lourdes à interpréter. Cela en fait un indicateur complémentaire pour repérer des zones “difficiles” dans un corpus, à condition de garder en tête qu’elle dépend toujours du modèle utilisé et du type de données sur lesquelles il a été entraîné. 

3. ## Implications pour la lisibilité et le traitement automatique 

La lisibilité est donc une mesure prédictive de la difficulté à comprendre un texte au sein d’un public donné, le plus souvent des apprenants en milieu scolaire. Son interprétation est basée sur des indices de complexité, historiquement uniquement de surface, corrélés avec une annotation de « jugement » humain pour en définir le degré de difficulté (Aissa et al. 2025).   
Le concept de lisibilité est interdépendant de celui de la complexité, il n’est pas possible de parler de lisibilité sans aborder celui de la complexité, cette dernière analyse plutôt les propriétés marquées linguistiquement, lesquelles permettent de savoir si une phrase est plus ou moins difficile/coûteuse en analyse. La lisibilité en revanche aborde en particulier l’interprétation globale d’un texte, du point de vue des lecteurs, ce n’est donc pas un critère objectivable, il dépend du public cible des personnes qui ont annoté le texte. Les avancées les plus récentes (Aissa et al. 2025\) reposent essentiellement sur l’intégration de ces méthodes d’application directement dans des modèles neuronaux de supervision, en particulier de type transformers qui sont spécifiquement entraînés sur des textes à plusieurs niveaux de difficulté.  
Nous pouvons prendre l’exemple de CamemBERT, transformer de type neuronal francophone basé sur BERT, qui permet la prédiction implicite d’indices de complexité. Aissa et al. (2025) ont enrichi ledit modèle avec des paramètres syntaxiques et l’ont entraîné à partir de corpus annotés localement (segments, phrases, passages), pour localiser les zones pertinentes du texte, en particulier les zones « difficiles ». Cette annotation est basée autour d’un jugement du type : « cette phrase est plus ou moins difficile » et est introduite après l’analyse de la complexité syntaxique (Lu, 2010). On peut alors estimer que lisibilité est une agrégation des différents indices de complexité, mais à l’échelle “locale” du texte, notamment par exemple la longueur du texte par le nombre de phrases. Un score plus élevé indiquant un texte jugé plus difficile par le modèle (Aissa et al., 2025).  
Finalement, la lisibilité est un concept qui ne travaille principalement qu’autour des indices de complexité de surface. La lisibilité est donc plutôt une estimation de la difficulté perçue par les lecteurs et calculée à partir de mesures de complexités, et aujourd’hui modélisée par des modèles automatiques.

6. # **Conclusion**

Pour conclure, la complexité linguistique est un principe multidimensionnel, elle joue un rôle essentiel non seulement pour la compréhension globale, mais aussi pour le traitement de la langue, que cela soit d’ordre syntaxique, lexical, ou d’autres paramètres, tels que les dépendances ou la fréquence morpho-phonologique. En effet, les études confirment (notamment ALSI avec ses 19 critères de complexité) que si la longueur des phrases et des mots compte pour mesurer la complexité, elle n’est cependant pas suffisante pour avoir des résultats fiables. Néanmoins, des outils efficaces ont été développés pour standardiser l’annotation, notamment de la complexité, avec Universal Dependencies. Il y a encore de nombreux travaux à accomplir pour obtenir des mesures de complexité qui soient réellement fiables et généralisables à l’ensemble de la linguistique. Mais s’il y a encore du travail à produire pour la fiabilité de ces mesures, il demeure tout de même que les outils dont on dispose actuellement semblent plutôt efficaces pour mesurer les phénomènes de complexité syntaxique.

Dans le cadre du projet CoSy, ces constats nous amènent à penser qu’il est préférable de combiner les jeux d’annotation à grande échelle, tels que Stanza(UD), avec des mesures plus fines et moins linguistiques telles que la perplexité, qui permettraient une analyse de corpus plus robuste.

Bibliographie 

Antoine, E., Bechet, F., Damnati, G., & Langlais, P. (2024). Étude des facteurs de complexité des modèles de langage dans une tâche de compréhension de lecture à l’aide d’une expérience contrôlée sémantiquement. In M. Balaguer, N. Bendahman, L.-M. Ho-dac, J. Mauclair, J. G Moreno, & J. Pinquier (Éds.), *Actes de la 31ème Conférence sur le Traitement Automatique des Langues Naturelles, volume 1 : Articles longs et prises de position* (p. 384‑396). ATALA and AFPC. [https://aclanthology.org/2024.jeptalnrecital-taln.27/](https://aclanthology.org/2024.jeptalnrecital-taln.27/)

Bentz, C., Gutierrez-Vasques, X., Sozinova, O., & Samardžić, T. (2023). Complexity trade-offs and equi-complexity in natural languages : A meta-analysis. *Linguistics Vanguard*, *9*(s1), 9‑25. [https://doi.org/10.1515/lingvan-2021-0054](https://doi.org/10.1515/lingvan-2021-0054)

Burov, I. (2022). Variables linguistiques et extralinguistiques pour la complexité des systèmes tonals : Le cas des langues africaines. *Italian Journal of Linguistics*, *33*(2), 59‑108. [https://doi.org/10.26346/1120-2726-176](https://doi.org/10.26346/1120-2726-176)

Dahl, Ö. (2004). *Growth and Maintenance of Linguistic Complexity*. John Benjamins Publishing Company.

Gala, N., François, T., Bernhard, D., & Fairon, C. (2014). *Un modèle pour prédire la complexité lexicale et graduer les mots.*

Hockett, C. F. (1958). *A COURSE IN MODERN LINGUISTICS*.   
[https://eric.ed.gov/?id=ED018170](https://eric.ed.gov/?id=ED018170)

Hudelot, C. (1980). *Qu’est-ce que la complexité syntaxique ? L’exemple de la relative*. La Linguistique, 16(2), 5‑41.

Journal of Experimental Psychology, *41*(6), 401‑410. [https://doi.org/10.1037/h0056020](https://doi.org/10.1037/h0056020)  
Loignon, G. (2021). ALSI : Un nouvel outil d’analyse automatisée de la complexité linguistique pour le français québécois. *Mesure et évaluation en éducation*, *44*(3), 29\. [https://doi.org/10.7202/1093065ar](https://doi.org/10.7202/1093065ar)

Lu, X. (2010). Automatic analysis of syntactic complexity in second language writing. *International Journal of Corpus Linguistics*, *15*(4), 474‑496.   
[https://doi.org/10.1075/ijcl.15.4.02lu](https://doi.org/10.1075/ijcl.15.4.02lu)

De Marneffe, M.-C., Manning, C. D., Nivre, J., & Zeman, D. (2021). *Universal Dependencies.* Computational Linguistics, 1‑54.  
 [https://doi.org/10.1162/coli\_a\_00402](https://doi.org/10.1162/coli_a_00402)

Romero-Barranco, J. (2020). Linguistic Complexity across Two Early Modern English Scientific Text Types. *Atlantis. Journal of the Spanish Association for Anglo-American Studies*, 50‑71. [https://doi.org/10.28914/Atlantis-2020-42.2.03](https://doi.org/10.28914/Atlantis-2020-42.2.03)

Aissa, W., Bañeras-Roux, T., Vanzeveren, E., Gao, L., Wilkens, R., & François, T. (2025). Assessing French Readability for Adults with Low Literacy : A Global and Local Perspective. In C. Christodoulopoulos, T. Chakraborty, C. Rose, & V. Peng (Éds.), *Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing* (p. 20528‑20550). Association for Computational Linguistics.   
[https://doi.org/10.18653/v1/2025.emnlp-main.1036](https://doi.org/10.18653/v1/2025.emnlp-main.1036)

Lee, B. W., Jang, Y. S., & Lee, J. (2021). Pushing on Text Readability Assessment : A Transformer Meets Handcrafted Linguistic Features. *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*, 10669‑10686.   
[https://doi.org/10.18653/v1/2021.emnlp-main.834](https://doi.org/10.18653/v1/2025.emnlp-main.1036)

